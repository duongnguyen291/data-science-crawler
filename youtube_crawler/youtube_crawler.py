"""
YouTube Comment Crawler s·ª≠ d·ª•ng YouTube Data API v3
"""

import os
import time
import json
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import logging

# Thi·∫øt l·∫≠p logging
from logger_config import get_crawler_logger
logger = get_crawler_logger()

class YouTubeCommentCrawler:
    def __init__(self, api_key: str):
        """
        Kh·ªüi t·∫°o YouTube Comment Crawler
        
        Args:
            api_key (str): YouTube Data API v3 key
        """
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.comments_data = []
        
    def extract_video_id(self, url: str) -> str:
        """
        Tr√≠ch xu·∫•t Video ID t·ª´ YouTube URL
        
        Args:
            url (str): YouTube URL
            
        Returns:
            str: Video ID
        """
        if 'youtube.com/watch?v=' in url:
            return url.split('v=')[1].split('&')[0]
        elif 'youtu.be/' in url:
            return url.split('youtu.be/')[1].split('?')[0]
        else:
            raise ValueError("Invalid YouTube URL format")
    
    def get_video_info(self, video_id: str) -> Dict:
        """
        L·∫•y th√¥ng tin c∆° b·∫£n c·ªßa video
        
        Args:
            video_id (str): Video ID
            
        Returns:
            Dict: Th√¥ng tin video
        """
        try:
            request = self.youtube.videos().list(
                part='snippet,statistics',
                id=video_id
            )
            response = request.execute()
            
            if response['items']:
                video = response['items'][0]
                return {
                    'video_id': video_id,
                    'title': video['snippet']['title'],
                    'channel_title': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': video['statistics'].get('viewCount', 0),
                    'like_count': video['statistics'].get('likeCount', 0),
                    'comment_count': video['statistics'].get('commentCount', 0)
                }
            else:
                logger.warning(f"Video {video_id} not found")
                return {}
                
        except HttpError as e:
            logger.error(f"Error getting video info: {e}")
            return {}
    
    def get_comments(self, video_id: str, max_comments: int = 100, order: str = 'time') -> List[Dict]:
        """
        L·∫•y comments t·ª´ video YouTube
        
        Args:
            video_id (str): Video ID
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa c·∫ßn l·∫•y
            order (str): Th·ª© t·ª± s·∫Øp x·∫øp ('time', 'relevance', 'rating')
            
        Returns:
            List[Dict]: Danh s√°ch comments
        """
        comments = []
        next_page_token = None
        total_fetched = 0
        
        logger.info(f"B·∫Øt ƒë·∫ßu crawl comments cho video: {video_id}")
        
        while total_fetched < max_comments:
            try:
                # L·∫•y comment threads (top-level comments)
                request = self.youtube.commentThreads().list(
                    part='snippet,replies',
                    videoId=video_id,
                    maxResults=min(100, max_comments - total_fetched),  # YouTube API limit: 100 per request
                    pageToken=next_page_token,
                    order=order  # S·∫Øp x·∫øp theo th·ª© t·ª± ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
                )
                
                response = request.execute()
                
                for item in response['items']:
                    if total_fetched >= max_comments:
                        break
                        
                    # X·ª≠ l√Ω top-level comment
                    top_comment = self._process_comment(item['snippet']['topLevelComment'], video_id)
                    comments.append(top_comment)
                    total_fetched += 1
                    
                    # X·ª≠ l√Ω replies n·∫øu c√≥
                    if 'replies' in item and total_fetched < max_comments:
                        for reply in item['replies']['comments'][:5]:  # Gi·ªõi h·∫°n 5 replies per comment
                            if total_fetched >= max_comments:
                                break
                            reply_comment = self._process_comment(reply, video_id, is_reply=True)
                            reply_comment['parent_comment_id'] = top_comment['comment_id']
                            comments.append(reply_comment)
                            total_fetched += 1
                
                # Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break
                    
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(0.1)
                
            except HttpError as e:
                logger.error(f"Error fetching comments: {e}")
                if e.resp.status == 403:
                    logger.error("API quota exceeded or access denied")
                    break
                time.sleep(1)
                
        logger.info(f"ƒê√£ crawl ƒë∆∞·ª£c {len(comments)} comments")
        return comments
    
    def _process_comment(self, comment_data: Dict, video_id: str, is_reply: bool = False) -> Dict:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu comment t·ª´ API response
        
        Args:
            comment_data (Dict): D·ªØ li·ªáu comment t·ª´ API
            video_id (str): Video ID
            is_reply (bool): C√≥ ph·∫£i l√† reply kh√¥ng
            
        Returns:
            Dict: Comment ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        snippet = comment_data['snippet']
        
        return {
            'comment_id': comment_data['id'],
            'post_id': video_id,
            'platform': 'YouTube',
            'author_name': snippet['authorDisplayName'],
            'author_id': snippet['authorChannelId']['value'] if 'authorChannelId' in snippet else None,
            'comment_text': snippet['textDisplay'],
            'published_at': snippet['publishedAt'],
            'like_count': snippet['likeCount'],
            'reply_count': snippet.get('totalReplyCount', 0),
            'sentiment_label': None,  # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau
            'sentiment_score': None,  # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau
            'crawled_at': datetime.now().isoformat(),
            'is_reply': is_reply,
            'parent_comment_id': None
        }
    
    def crawl_video(self, video_url: str, max_comments: int = 100, order: str = 'time') -> Dict:
        """
        Crawl comments t·ª´ m·ªôt video YouTube
        
        Args:
            video_url (str): URL c·ªßa video YouTube
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa
            order (str): Th·ª© t·ª± s·∫Øp x·∫øp ('time', 'relevance', 'rating')
            
        Returns:
            Dict: K·∫øt qu·∫£ crawl
        """
        try:
            # Tr√≠ch xu·∫•t video ID
            video_id = self.extract_video_id(video_url)
            logger.info(f"Video ID: {video_id}")
            
            # L·∫•y th√¥ng tin video
            video_info = self.get_video_info(video_id)
            if not video_info:
                return {'success': False, 'error': 'Video not found'}
            
            # L·∫•y comments
            comments = self.get_comments(video_id, max_comments, order)
            
            return {
                'success': True,
                'video_info': video_info,
                'comments': comments,
                'total_comments': len(comments)
            }
            
        except Exception as e:
            logger.error(f"Error crawling video: {e}")
            return {'success': False, 'error': str(e)}
    
    def save_to_csv(self, data: List[Dict], filename: str = None):
        """
        L∆∞u d·ªØ li·ªáu comments v√†o file CSV
        
        Args:
            data (List[Dict]): D·ªØ li·ªáu comments
            filename (str): T√™n file (t·ª± ƒë·ªông t·∫°o n·∫øu None)
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_comments_{timestamp}.csv"
        
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8')
        logger.info(f"ƒê√£ l∆∞u {len(data)} comments v√†o file: {filename}")
        return filename
    
    def save_to_json(self, data: List[Dict], filename: str = None):
        """
        L∆∞u d·ªØ li·ªáu comments v√†o file JSON
        
        Args:
            data (List[Dict]): D·ªØ li·ªáu comments
            filename (str): T√™n file (t·ª± ƒë·ªông t·∫°o n·∫øu None)
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_comments_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ƒê√£ l∆∞u {len(data)} comments v√†o file: {filename}")
        return filename
    
    def get_top_comments(self, video_url: str, max_comments: int = 50, min_likes: int = 0) -> Dict:
        """
        L·∫•y c√°c comment c√≥ l∆∞·ª£t like cao nh·∫•t
        
        Args:
            video_url (str): URL c·ªßa video YouTube
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa
            min_likes (int): S·ªë l∆∞·ª£t like t·ªëi thi·ªÉu
            
        Returns:
            Dict: K·∫øt qu·∫£ crawl v·ªõi comments ƒë∆∞·ª£c s·∫Øp x·∫øp theo like
        """
        try:
            # Tr√≠ch xu·∫•t video ID
            video_id = self.extract_video_id(video_url)
            logger.info(f"L·∫•y top comments cho video: {video_id}")
            
            # L·∫•y th√¥ng tin video
            video_info = self.get_video_info(video_id)
            if not video_info:
                return {'success': False, 'error': 'Video not found'}
            
            # L·∫•y comments v·ªõi order='relevance' ƒë·ªÉ ∆∞u ti√™n comments c√≥ relevance cao
            comments = self.get_comments(video_id, max_comments * 2, order='relevance')
            
            # L·ªçc comments c√≥ like >= min_likes
            filtered_comments = [c for c in comments if c['like_count'] >= min_likes]
            
            # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£t like gi·∫£m d·∫ßn
            sorted_comments = sorted(filtered_comments, key=lambda x: x['like_count'], reverse=True)
            
            # L·∫•y top comments
            top_comments = sorted_comments[:max_comments]
            
            logger.info(f"ƒê√£ l·∫•y {len(top_comments)} top comments (min_likes={min_likes})")
            
            return {
                'success': True,
                'video_info': video_info,
                'comments': top_comments,
                'total_comments': len(top_comments),
                'min_likes': min_likes,
                'order': 'top_liked'
            }
            
        except Exception as e:
            logger.error(f"Error getting top comments: {e}")
            return {'success': False, 'error': str(e)}


def main():
    """
    H√†m main ƒë·ªÉ test crawler
    """
    # Thay th·∫ø b·∫±ng API key th·ª±c c·ªßa b·∫°n
    API_KEY = "YOUR_API_KEY_HERE"
    
    if API_KEY == "YOUR_API_KEY_HERE":
        print("Vui l√≤ng thay th·∫ø API_KEY b·∫±ng API key th·ª±c c·ªßa b·∫°n!")
        print("B·∫°n c√≥ th·ªÉ l·∫•y API key t·ª´: https://console.cloud.google.com/")
        return
    
    # Kh·ªüi t·∫°o crawler
    crawler = YouTubeCommentCrawler(API_KEY)
    
    # Test v·ªõi m·ªôt video c·ª• th·ªÉ
    test_video_url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ"  # Rick Roll video
    
    print(f"B·∫Øt ƒë·∫ßu crawl comments t·ª´: {test_video_url}")
    
    # Crawl comments
    result = crawler.crawl_video(test_video_url, max_comments=50)
    
    if result['success']:
        print(f"‚úÖ Crawl th√†nh c√¥ng!")
        print(f"üìπ Video: {result['video_info']['title']}")
        print(f"üìä T·ªïng s·ªë comments: {result['total_comments']}")
        
        # Hi·ªÉn th·ªã m·ªôt v√†i comments m·∫´u
        print("\nüìù M·ªôt v√†i comments m·∫´u:")
        for i, comment in enumerate(result['comments'][:3]):
            print(f"{i+1}. {comment['author_name']}: {comment['comment_text'][:100]}...")
        
        # L∆∞u d·ªØ li·ªáu
        csv_file = crawler.save_to_csv(result['comments'])
        json_file = crawler.save_to_json(result['comments'])
        
        print(f"\nüíæ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o:")
        print(f"   - CSV: {csv_file}")
        print(f"   - JSON: {json_file}")
        
    else:
        print(f"‚ùå Crawl th·∫•t b·∫°i: {result['error']}")


if __name__ == "__main__":
    main()