"""
YouTube Comment Crawler s·ª≠ d·ª•ng YouTube Data API v3
"""

import os
import time
import json
import argparse
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional, Any
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import logging

# Thi·∫øt l·∫≠p logging
from logger_config import get_crawler_logger
logger = get_crawler_logger()

class YouTubeCommentCrawler:
    def __init__(self, api_key: str):
        """
        Kh·ªüi t·∫°o YouTube Comment Crawler
        
        Args:
            api_key (str): YouTube Data API v3 key
        """
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.comments_data = []
        
    def extract_video_id(self, url: str) -> str:
        """
        Tr√≠ch xu·∫•t Video ID t·ª´ YouTube URL
        
        Args:
            url (str): YouTube URL
            
        Returns:
            str: Video ID
        """
        if 'youtube.com/watch?v=' in url:
            return url.split('v=')[1].split('&')[0]
        elif 'youtu.be/' in url:
            return url.split('youtu.be/')[1].split('?')[0]
        else:
            raise ValueError("Invalid YouTube URL format")
    
    def get_video_info(self, video_id: str) -> Dict:
        """
        L·∫•y th√¥ng tin c∆° b·∫£n c·ªßa video
        
        Args:
            video_id (str): Video ID
            
        Returns:
            Dict: Th√¥ng tin video
        """
        try:
            request = self.youtube.videos().list(
                part='snippet,statistics',
                id=video_id
            )
            response = request.execute()
            
            if response['items']:
                video = response['items'][0]
                return {
                    'video_id': video_id,
                    'title': video['snippet']['title'],
                    'channel_title': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': video['statistics'].get('viewCount', 0),
                    'like_count': video['statistics'].get('likeCount', 0),
                    'comment_count': video['statistics'].get('commentCount', 0)
                }
            else:
                logger.warning(f"Video {video_id} not found")
                return {}
                
        except HttpError as e:
            logger.error(f"Error getting video info: {e}")
            return {}
    
    def _count_words(self, text: str) -> int:
        """
        ƒê·∫øm s·ªë t·ª´ trong text
        
        Args:
            text (str): Text c·∫ßn ƒë·∫øm
            
        Returns:
            int: S·ªë t·ª´
        """
        if not text or not isinstance(text, str):
            return 0
        # T√°ch theo kho·∫£ng tr·∫Øng v√† lo·∫°i b·ªè c√°c ph·∫ßn t·ª≠ r·ªóng
        words = [w.strip() for w in text.split() if w.strip()]
        return len(words)
    
    def _filter_by_word_count(self, comments: List[Dict], min_words: Optional[int] = None, 
                              max_words: Optional[int] = None) -> List[Dict]:
        """
        L·ªçc comments theo s·ªë l∆∞·ª£ng t·ª´
        
        Args:
            comments (List[Dict]): Danh s√°ch comments
            min_words (Optional[int]): S·ªë t·ª´ t·ªëi thi·ªÉu (None = kh√¥ng gi·ªõi h·∫°n)
            max_words (Optional[int]): S·ªë t·ª´ t·ªëi ƒëa (None = kh√¥ng gi·ªõi h·∫°n)
            
        Returns:
            List[Dict]: Danh s√°ch comments ƒë√£ ƒë∆∞·ª£c l·ªçc
        """
        if min_words is None and max_words is None:
            return comments
        
        filtered = []
        for comment in comments:
            word_count = self._count_words(comment.get('comment_text', ''))
            
            # Ki·ªÉm tra c·∫≠n d∆∞·ªõi
            if min_words is not None and word_count < min_words:
                continue
            
            # Ki·ªÉm tra c·∫≠n tr√™n
            if max_words is not None and word_count > max_words:
                continue
            
            filtered.append(comment)
        
        return filtered
    
    def get_comments(self, video_id: str, max_comments: int = 100, order: str = 'time',
                     min_words: Optional[int] = None, max_words: Optional[int] = None) -> List[Dict]:
        """
        L·∫•y comments t·ª´ video YouTube
        
        Args:
            video_id (str): Video ID
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa c·∫ßn l·∫•y
            order (str): Th·ª© t·ª± s·∫Øp x·∫øp ('time', 'relevance', 'rating')
            min_words (Optional[int]): S·ªë t·ª´ t·ªëi thi·ªÉu (None = kh√¥ng gi·ªõi h·∫°n)
            max_words (Optional[int]): S·ªë t·ª´ t·ªëi ƒëa (None = kh√¥ng gi·ªõi h·∫°n)
            
        Returns:
            List[Dict]: Danh s√°ch comments
        """
        next_page_token = None
        
        logger.info(f"B·∫Øt ƒë·∫ßu crawl comments cho video: {video_id}")
        if min_words is not None or max_words is not None:
            logger.info(f"Filter comments: min_words={min_words}, max_words={max_words}")
        
        # TƒÉng s·ªë l∆∞·ª£ng fetch ƒë·ªÉ b√π cho vi·ªác filter
        # ∆Ø·ªõc t√≠nh: n·∫øu filter th√¨ c·∫ßn fetch nhi·ªÅu h∆°n ƒë·ªÉ ƒë·ªß s·ªë l∆∞·ª£ng
        fetch_multiplier = 2 if (min_words is not None or max_words is not None) else 1
        target_fetch = max_comments * fetch_multiplier
        raw_comments = []
        
        while len(raw_comments) < target_fetch:
            try:
                # L·∫•y comment threads (top-level comments)
                # T√≠nh s·ªë l∆∞·ª£ng c√≤n c·∫ßn fetch, t·ªëi ƒëa 100 m·ªói request (YouTube API limit)
                remaining = target_fetch - len(raw_comments)
                request = self.youtube.commentThreads().list(
                    part='snippet,replies',
                    videoId=video_id,
                    maxResults=min(100, remaining),  # YouTube API limit: 100 per request
                    pageToken=next_page_token,
                    order=order  # S·∫Øp x·∫øp theo th·ª© t·ª± ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
                )
                
                response = request.execute()
                
                for item in response['items']:
                    if len(raw_comments) >= target_fetch:
                        break
                        
                    # X·ª≠ l√Ω top-level comment
                    top_comment = self._process_comment(item['snippet']['topLevelComment'], video_id)
                    raw_comments.append(top_comment)
                    
                    # X·ª≠ l√Ω replies n·∫øu c√≥
                    if 'replies' in item:
                        for reply in item['replies']['comments'][:5]:  # Gi·ªõi h·∫°n 5 replies per comment
                            if len(raw_comments) >= target_fetch:
                                break
                            reply_comment = self._process_comment(reply, video_id, is_reply=True)
                            reply_comment['parent_comment_id'] = top_comment['comment_id']
                            raw_comments.append(reply_comment)
                
                # Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break
                    
                # Delay ƒë·ªÉ tr√°nh rate limit
                time.sleep(0.1)
                
            except HttpError as e:
                logger.error(f"Error fetching comments: {e}")
                if e.resp.status == 403:
                    logger.error("API quota exceeded or access denied")
                    break
                time.sleep(1)
        
        # Filter comments theo s·ªë l∆∞·ª£ng t·ª´
        comments = self._filter_by_word_count(raw_comments, min_words, max_words)
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng comments cu·ªëi c√πng
        comments = comments[:max_comments]
        
        logger.info(f"ƒê√£ crawl ƒë∆∞·ª£c {len(raw_comments)} comments, sau filter c√≤n {len(comments)} comments")
        return comments
    
    def _process_comment(self, comment_data: Dict, video_id: str, is_reply: bool = False) -> Dict:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu comment t·ª´ API response
        
        Args:
            comment_data (Dict): D·ªØ li·ªáu comment t·ª´ API
            video_id (str): Video ID
            is_reply (bool): C√≥ ph·∫£i l√† reply kh√¥ng
            
        Returns:
            Dict: Comment ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        snippet = comment_data['snippet']
        
        return {
            'comment_id': comment_data['id'],
            'post_id': video_id,
            'platform': 'YouTube',
            'author_name': snippet['authorDisplayName'],
            'author_id': snippet['authorChannelId']['value'] if 'authorChannelId' in snippet else None,
            'comment_text': snippet['textDisplay'],
            'published_at': snippet['publishedAt'],
            'like_count': snippet['likeCount'],
            'reply_count': snippet.get('totalReplyCount', 0),
            'sentiment_label': None,  # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau
            'sentiment_score': None,  # S·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau
            'crawled_at': datetime.now().isoformat(),
            'is_reply': is_reply,
            'parent_comment_id': None
        }
    
    def crawl_video(self, video_url: str, max_comments: int = 100, order: str = 'time',
                   min_words: Optional[int] = None, max_words: Optional[int] = None) -> Dict:
        """
        Crawl comments t·ª´ m·ªôt video YouTube
        
        Args:
            video_url (str): URL c·ªßa video YouTube
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa
            order (str): Th·ª© t·ª± s·∫Øp x·∫øp ('time', 'relevance', 'rating')
            min_words (Optional[int]): S·ªë t·ª´ t·ªëi thi·ªÉu (None = kh√¥ng gi·ªõi h·∫°n)
            max_words (Optional[int]): S·ªë t·ª´ t·ªëi ƒëa (None = kh√¥ng gi·ªõi h·∫°n)
            
        Returns:
            Dict: K·∫øt qu·∫£ crawl
        """
        try:
            # Tr√≠ch xu·∫•t video ID
            video_id = self.extract_video_id(video_url)
            logger.info(f"Video ID: {video_id}")
            
            # L·∫•y th√¥ng tin video
            video_info = self.get_video_info(video_id)
            if not video_info:
                return {'success': False, 'error': 'Video not found'}
            
            # L·∫•y comments
            comments = self.get_comments(video_id, max_comments, order, min_words, max_words)
            
            return {
                'success': True,
                'video_info': video_info,
                'comments': comments,
                'total_comments': len(comments)
            }
            
        except Exception as e:
            logger.error(f"Error crawling video: {e}")
            return {'success': False, 'error': str(e)}
    
    def crawl_from_csv(
        self,
        csv_path: str,
        url_column: str = 'url',
        max_comments: int = 100,
        order: str = 'time',
        delay: float = 0.2,
        limit: Optional[int] = None,
        deduplicate_urls: bool = True,
        min_likes: int = 0,
        min_words: Optional[int] = None,
        max_words: Optional[int] = None
    ) -> Dict:
        """
        Crawl comments cho danh s√°ch video trong file CSV
        
        Args:
            csv_path (str): ƒê∆∞·ªùng d·∫´n t·ªõi file CSV ch·ª©a danh s√°ch video
            url_column (str): T√™n c·ªôt ch·ª©a URL video (m·∫∑c ƒë·ªãnh: 'url')
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa cho m·ªói video
            order (str): Th·ª© t·ª± s·∫Øp x·∫øp comments
            delay (float): Th·ªùi gian ngh·ªâ gi·ªØa m·ªói video (gi√¢y)
            limit (int): Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng video c·∫ßn crawl (None = t·∫•t c·∫£)
            deduplicate_urls (bool): C√≥ b·ªè qua URL tr√πng l·∫∑p kh√¥ng
            min_likes (int): S·ªë l∆∞·ª£t like t·ªëi thi·ªÉu (0 = disabled)
            min_words (Optional[int]): S·ªë t·ª´ t·ªëi thi·ªÉu (None = kh√¥ng gi·ªõi h·∫°n)
            max_words (Optional[int]): S·ªë t·ª´ t·ªëi ƒëa (None = kh√¥ng gi·ªõi h·∫°n)
        
        Returns:
            Dict: T·ªïng h·ª£p k·∫øt qu·∫£ crawl
        """
        if not os.path.isfile(csv_path):
            logger.error(f"CSV file not found: {csv_path}")
            return {'success': False, 'error': f'CSV file not found: {csv_path}'}
        
        df = pd.read_csv(csv_path)
        
        if url_column not in df.columns:
            error_msg = f"Column '{url_column}' not found in CSV"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}
        
        context_columns = [col for col in df.columns if col != url_column]
        videos: List[Dict[str, Any]] = []
        seen_urls = set()
        
        for _, row in df.iterrows():
            raw_url = row.get(url_column)
            if pd.isna(raw_url):
                continue
            video_url = str(raw_url).strip()
            if not video_url:
                continue
            if deduplicate_urls and video_url in seen_urls:
                continue
            seen_urls.add(video_url)
            
            metadata = {}
            for col in context_columns:
                value = row.get(col)
                metadata[f"source_{col}"] = None if pd.isna(value) else value
            videos.append({'url': video_url, 'metadata': metadata})
        
        if not videos:
            error_msg = f"No valid URLs found in column '{url_column}'"
            logger.error(error_msg)
            return {'success': False, 'error': error_msg}
        
        if limit is not None and limit > 0:
            videos = videos[:limit]
        
        crawl_summary = []
        failed_urls = []
        total_comments = 0
        
        logger.info(f"B·∫Øt ƒë·∫ßu crawl t·ª´ CSV: {csv_path}")
        logger.info(f"T·ªïng s·ªë video c·∫ßn crawl: {len(videos)}")
        
        for idx, video_entry in enumerate(videos, start=1):
            video_url = video_entry['url']
            metadata = video_entry['metadata']
            
            logger.info(f"[{idx}/{len(videos)}] Crawl video: {video_url}")
            # S·ª≠ d·ª•ng get_top_comments n·∫øu min_likes > 0, ng∆∞·ª£c l·∫°i d√πng crawl_video
            if min_likes > 0:
                result = self.get_top_comments(video_url, max_comments, min_likes, min_words, max_words)
            else:
                result = self.crawl_video(video_url, max_comments=max_comments, order=order, 
                                        min_words=min_words, max_words=max_words)
            
            summary_item = {
                'url': video_url,
                'metadata': metadata,
                'success': result['success'],
                'total_comments': result.get('total_comments', 0)
            }
            
            if result['success']:
                # G·∫Øn metadata v√†o comments
                comments = result['comments']
                if metadata:
                    for comment in comments:
                        comment.update(metadata)
                
                self.comments_data.extend(comments)
                total_comments += len(comments)
                summary_item['video_info'] = result['video_info']
            else:
                summary_item['error'] = result.get('error', 'Unknown error')
                failed_urls.append(video_url)
            
            crawl_summary.append(summary_item)
            time.sleep(max(delay, 0))
        
        logger.info(f"Ho√†n th√†nh crawl t·ª´ CSV. T·ªïng s·ªë comments: {total_comments}")
        if failed_urls:
            logger.warning(f"Crawl th·∫•t b·∫°i cho {len(failed_urls)} video: {failed_urls}")
        
        return {
            'success': len(failed_urls) == 0,
            'summary': crawl_summary,
            'failed_urls': failed_urls,
            'total_videos': len(videos),
            'total_comments': total_comments,
            'aggregated_comments': self.comments_data
        }
    
    def save_to_csv(self, data: List[Dict], filename: str = None):
        """
        L∆∞u d·ªØ li·ªáu comments v√†o file CSV
        
        Args:
            data (List[Dict]): D·ªØ li·ªáu comments
            filename (str): T√™n file (t·ª± ƒë·ªông t·∫°o n·∫øu None)
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_comments_{timestamp}.csv"
        
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8')
        logger.info(f"ƒê√£ l∆∞u {len(data)} comments v√†o file: {filename}")
        return filename
    
    def save_to_json(self, data: List[Dict], filename: str = None):
        """
        L∆∞u d·ªØ li·ªáu comments v√†o file JSON
        
        Args:
            data (List[Dict]): D·ªØ li·ªáu comments
            filename (str): T√™n file (t·ª± ƒë·ªông t·∫°o n·∫øu None)
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"youtube_comments_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ƒê√£ l∆∞u {len(data)} comments v√†o file: {filename}")
        return filename
    
    def get_top_comments(self, video_url: str, max_comments: int = 50, min_likes: int = 0,
                        min_words: Optional[int] = None, max_words: Optional[int] = None) -> Dict:
        """
        L·∫•y c√°c comment c√≥ l∆∞·ª£t like cao nh·∫•t
        
        Args:
            video_url (str): URL c·ªßa video YouTube
            max_comments (int): S·ªë l∆∞·ª£ng comment t·ªëi ƒëa
            min_likes (int): S·ªë l∆∞·ª£t like t·ªëi thi·ªÉu
            min_words (Optional[int]): S·ªë t·ª´ t·ªëi thi·ªÉu (None = kh√¥ng gi·ªõi h·∫°n)
            max_words (Optional[int]): S·ªë t·ª´ t·ªëi ƒëa (None = kh√¥ng gi·ªõi h·∫°n)
            
        Returns:
            Dict: K·∫øt qu·∫£ crawl v·ªõi comments ƒë∆∞·ª£c s·∫Øp x·∫øp theo like
        """
        try:
            # Tr√≠ch xu·∫•t video ID
            video_id = self.extract_video_id(video_url)
            logger.info(f"L·∫•y top comments cho video: {video_id}")
            
            # L·∫•y th√¥ng tin video
            video_info = self.get_video_info(video_id)
            if not video_info:
                return {'success': False, 'error': 'Video not found'}
            
            # L·∫•y comments v·ªõi order='relevance' ƒë·ªÉ ∆∞u ti√™n comments c√≥ relevance cao
            # Filter theo word count ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong get_comments
            comments = self.get_comments(video_id, max_comments * 2, order='relevance', 
                                        min_words=min_words, max_words=max_words)
            
            # L·ªçc comments c√≥ like >= min_likes
            filtered_comments = [c for c in comments if c['like_count'] >= min_likes]
            
            # S·∫Øp x·∫øp theo s·ªë l∆∞·ª£t like gi·∫£m d·∫ßn
            sorted_comments = sorted(filtered_comments, key=lambda x: x['like_count'], reverse=True)
            
            # L·∫•y top comments
            top_comments = sorted_comments[:max_comments]
            
            logger.info(f"ƒê√£ l·∫•y {len(top_comments)} top comments (min_likes={min_likes})")
            
            return {
                'success': True,
                'video_info': video_info,
                'comments': top_comments,
                'total_comments': len(top_comments),
                'min_likes': min_likes,
                'order': 'top_liked'
            }
            
        except Exception as e:
            logger.error(f"Error getting top comments: {e}")
            return {'success': False, 'error': str(e)}


def parse_args():
    parser = argparse.ArgumentParser(description="YouTube Comment Crawler CLI")
    parser.add_argument('--api-key', help='YouTube Data API v3 key (ho·∫∑c ƒë·∫∑t env YOUTUBE_API_KEY)')
    parser.add_argument('--video-url', help='URL video YouTube c·∫ßn crawl')
    parser.add_argument('--csv-path', help='ƒê∆∞·ªùng d·∫´n t·ªõi file CSV ch·ª©a danh s√°ch video')
    parser.add_argument('--url-column', default='url', help="T√™n c·ªôt URL trong CSV (m·∫∑c ƒë·ªãnh: 'url')")
    parser.add_argument('--max-comments', type=int, default=100, help='S·ªë comment t·ªëi ƒëa m·ªói video')
    parser.add_argument('--order', choices=['time', 'relevance', 'rating'], default='time',
                        help='Th·ª© t·ª± l·∫•y comments')
    parser.add_argument('--delay', type=float, default=0.2, help='Th·ªùi gian ngh·ªâ gi·ªØa c√°c video (gi√¢y)')
    parser.add_argument('--limit', type=int, help='Gi·ªõi h·∫°n s·ªë video c·∫ßn crawl khi d√πng CSV')
    parser.add_argument('--no-dedupe', action='store_true', help='Kh√¥ng b·ªè qua URL tr√πng l·∫∑p trong CSV')
    parser.add_argument('--min-words', type=int, help='S·ªë t·ª´ t·ªëi thi·ªÉu c·ªßa comment (None = kh√¥ng gi·ªõi h·∫°n)')
    parser.add_argument('--max-words', type=int, help='S·ªë t·ª´ t·ªëi ƒëa c·ªßa comment (None = kh√¥ng gi·ªõi h·∫°n)')
    parser.add_argument('--output-csv', help='File CSV ƒë·ªÉ l∆∞u to√†n b·ªô comments k·∫øt qu·∫£')
    parser.add_argument('--output-json', help='File JSON ƒë·ªÉ l∆∞u to√†n b·ªô comments k·∫øt qu·∫£')
    return parser.parse_args()


def main():
    """
    CLI cho YouTube Comment Crawler.
    H·ªó tr·ª£ crawl m·ªôt video ho·∫∑c crawl h√†ng lo·∫°t t·ª´ file CSV.
    """
    args = parse_args()
    
    api_key = args.api_key or os.getenv('YOUTUBE_API_KEY') or "YOUR_API_KEY_HERE"
    if api_key == "YOUR_API_KEY_HERE":
        print("‚ùå Thi·∫øu API key. D√πng --api-key ho·∫∑c ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng YOUTUBE_API_KEY.")
        return
    
    if not args.video_url and not args.csv_path:
        print("‚ùå Vui l√≤ng cung c·∫•p --video-url ho·∫∑c --csv-path.")
        return
    
    crawler = YouTubeCommentCrawler(api_key)
    aggregated_comments: List[Dict] = []
    
    if args.video_url:
        print(f"B·∫Øt ƒë·∫ßu crawl video: {args.video_url}")
        result = crawler.crawl_video(args.video_url, max_comments=args.max_comments, order=args.order,
                                   min_words=args.min_words, max_words=args.max_words)
        if result['success']:
            aggregated_comments = result['comments']
            print(f"‚úÖ Crawl th√†nh c√¥ng {result['total_comments']} comments.")
            print(f"üìπ {result['video_info']['title']} ({result['video_info']['channel_title']})")
        else:
            print(f"‚ùå Crawl th·∫•t b·∫°i: {result['error']}")
            return
    
    if args.csv_path:
        csv_result = crawler.crawl_from_csv(
            csv_path=args.csv_path,
            url_column=args.url_column,
            max_comments=args.max_comments,
            order=args.order,
            delay=args.delay,
            limit=args.limit,
            deduplicate_urls=not args.no_dedupe,
            min_words=args.min_words,
            max_words=args.max_words
        )
        
        if not csv_result['success']:
            print("‚ö†Ô∏è Ho√†n th√†nh v·ªõi m·ªôt s·ªë l·ªói.")
        else:
            print("‚úÖ ƒê√£ crawl xong danh s√°ch video.")
        
        print(f"üìä T·ªïng s·ªë video x·ª≠ l√Ω: {csv_result['total_videos']}")
        print(f"üí¨ T·ªïng s·ªë comments thu ƒë∆∞·ª£c: {csv_result['total_comments']}")
        if csv_result['failed_urls']:
            print(f"‚ùó Video l·ªói: {len(csv_result['failed_urls'])}")
            for failed in csv_result['failed_urls']:
                print(f"   - {failed}")
        
        aggregated_comments = csv_result['aggregated_comments']
    
    # L∆∞u k·∫øt qu·∫£ n·∫øu c·∫ßn
    if aggregated_comments:
        if args.output_csv:
            crawler.save_to_csv(aggregated_comments, args.output_csv)
        if args.output_json:
            crawler.save_to_json(aggregated_comments, args.output_json)
    else:
        print("‚ÑπÔ∏è Kh√¥ng c√≥ d·ªØ li·ªáu comment ƒë·ªÉ l∆∞u.")


if __name__ == "__main__":
    main()