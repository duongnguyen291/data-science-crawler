"""
Script l√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu comments
"""

import re
import pandas as pd
import numpy as np
from typing import List, Dict, Optional
import logging
from datetime import datetime

# Thi·∫øt l·∫≠p logging
from logger_config import get_cleaner_logger
logger = get_cleaner_logger()

class CommentDataCleaner:
    def __init__(self):
        """
        Kh·ªüi t·∫°o Comment Data Cleaner
        """
        # Regex patterns ƒë·ªÉ l√†m s·∫°ch text
        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        self.mention_pattern = re.compile(r'@\w+')
        self.hashtag_pattern = re.compile(r'#\w+')
        self.emoji_pattern = re.compile(r'[^\w\s.,!?;:()\-]')
        self.extra_spaces_pattern = re.compile(r'\s+')
        
        # Danh s√°ch t·ª´ ng·ªØ kh√¥ng mong mu·ªën
        self.stop_words = {
            'en': ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'],
            'vi': ['v√†', 'c·ªßa', 'v·ªõi', 't·ª´', 'trong', 'tr√™n', 'd∆∞·ªõi', 'v·ªÅ', 'cho', 'ƒë·ªÉ', 'khi', 'n·∫øu', 'nh∆∞ng', 'ho·∫∑c', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 's·∫Ω', 'ƒë√£', 'ƒëang', 't√¥i', 'b·∫°n', 'anh', 'ch·ªã', 'em', 'ch√∫ng', 'h·ªç', 'n√†y', 'ƒë√≥', 'kia', 'ƒë√¢y', 'ƒë√≥']
        }
    
    def clean_comment_text(self, text: str, remove_urls: bool = True, 
                          remove_mentions: bool = True, remove_hashtags: bool = True,
                          remove_emojis: bool = False, normalize_spaces: bool = True) -> str:
        """
        L√†m s·∫°ch text comment
        
        Args:
            text (str): Text g·ªëc
            remove_urls (bool): C√≥ x√≥a URLs kh√¥ng
            remove_mentions (bool): C√≥ x√≥a mentions (@username) kh√¥ng
            remove_hashtags (bool): C√≥ x√≥a hashtags (#tag) kh√¥ng
            remove_emojis (bool): C√≥ x√≥a emojis kh√¥ng
            normalize_spaces (bool): C√≥ chu·∫©n h√≥a kho·∫£ng tr·∫Øng kh√¥ng
            
        Returns:
            str: Text ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        if not text or pd.isna(text):
            return ""
        
        # Chuy·ªÉn v·ªÅ string v√† lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu cu·ªëi
        text = str(text).strip()
        
        # X√≥a URLs
        if remove_urls:
            text = self.url_pattern.sub('', text)
        
        # X√≥a mentions
        if remove_mentions:
            text = self.mention_pattern.sub('', text)
        
        # X√≥a hashtags
        if remove_hashtags:
            text = self.hashtag_pattern.sub('', text)
        
        # X√≥a emojis (t√πy ch·ªçn)
        if remove_emojis:
            text = self.emoji_pattern.sub('', text)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        if normalize_spaces:
            text = self.extra_spaces_pattern.sub(' ', text)
        
        return text.strip()
    
    def detect_language(self, text: str) -> str:
        """
        Ph√°t hi·ªán ng√¥n ng·ªØ c·ªßa comment (ƒë∆°n gi·∫£n)
        
        Args:
            text (str): Text comment
            
        Returns:
            str: 'vi' (ti·∫øng Vi·ªát) ho·∫∑c 'en' (ti·∫øng Anh)
        """
        if not text:
            return 'unknown'
        
        # ƒê·∫øm k√Ω t·ª± ti·∫øng Vi·ªát
        vietnamese_chars = len(re.findall(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', text.lower()))
        
        # ƒê·∫øm t·ª´ ti·∫øng Vi·ªát ph·ªï bi·∫øn
        vietnamese_words = ['v√†', 'c·ªßa', 'v·ªõi', 't·ª´', 'trong', 'tr√™n', 'd∆∞·ªõi', 'v·ªÅ', 'cho', 'ƒë·ªÉ', 'khi', 'n·∫øu', 'nh∆∞ng', 'ho·∫∑c', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 's·∫Ω', 'ƒë√£', 'ƒëang']
        vietnamese_word_count = sum(1 for word in vietnamese_words if word in text.lower())
        
        # N·∫øu c√≥ nhi·ªÅu k√Ω t·ª± ti·∫øng Vi·ªát ho·∫∑c t·ª´ ti·∫øng Vi·ªát
        if vietnamese_chars > 2 or vietnamese_word_count > 0:
            return 'vi'
        else:
            return 'en'
    
    def remove_stop_words(self, text: str, language: str = 'auto') -> str:
        """
        Lo·∫°i b·ªè stop words
        
        Args:
            text (str): Text g·ªëc
            language (str): Ng√¥n ng·ªØ ('vi', 'en', 'auto')
            
        Returns:
            str: Text ƒë√£ lo·∫°i b·ªè stop words
        """
        if not text:
            return ""
        
        # T·ª± ƒë·ªông ph√°t hi·ªán ng√¥n ng·ªØ
        if language == 'auto':
            language = self.detect_language(text)
        
        if language not in self.stop_words:
            return text
        
        words = text.lower().split()
        filtered_words = [word for word in words if word not in self.stop_words[language]]
        
        return ' '.join(filtered_words)
    
    def validate_comment(self, comment: Dict) -> Dict:
        """
        Ki·ªÉm tra v√† validate comment data
        
        Args:
            comment (Dict): Comment data
            
        Returns:
            Dict: Comment ƒë√£ ƒë∆∞·ª£c validate v√† l√†m s·∫°ch
        """
        cleaned_comment = comment.copy()
        
        # L√†m s·∫°ch comment text
        original_text = comment.get('comment_text', '')
        cleaned_text = self.clean_comment_text(original_text)
        
        # Ph√°t hi·ªán ng√¥n ng·ªØ
        language = self.detect_language(cleaned_text)
        
        # Lo·∫°i b·ªè stop words
        text_without_stopwords = self.remove_stop_words(cleaned_text, language)
        
        # C·∫≠p nh·∫≠t comment
        cleaned_comment.update({
            'comment_text': cleaned_text,
            'comment_text_clean': text_without_stopwords,
            'language': language,
            'text_length': len(cleaned_text),
            'word_count': len(cleaned_text.split()),
            'is_valid': len(cleaned_text.strip()) > 0,
            'cleaned_at': datetime.now().isoformat()
        })
        
        # Validate c√°c tr∆∞·ªùng kh√°c
        cleaned_comment['like_count'] = max(0, int(comment.get('like_count', 0)))
        cleaned_comment['reply_count'] = max(0, int(comment.get('reply_count', 0)))
        
        return cleaned_comment
    
    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        L√†m s·∫°ch to√†n b·ªô DataFrame
        
        Args:
            df (pd.DataFrame): DataFrame ch·ª©a comments
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        logger.info(f"B·∫Øt ƒë·∫ßu l√†m s·∫°ch {len(df)} comments...")
        
        # T·∫°o b·∫£n copy ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn data g·ªëc
        cleaned_df = df.copy()
        
        # L√†m s·∫°ch t·ª´ng comment
        cleaned_comments = []
        for idx, comment in df.iterrows():
            try:
                cleaned_comment = self.validate_comment(comment.to_dict())
                cleaned_comments.append(cleaned_comment)
            except Exception as e:
                logger.warning(f"L·ªói khi l√†m s·∫°ch comment {idx}: {e}")
                # Gi·ªØ nguy√™n comment g·ªëc n·∫øu c√≥ l·ªói
                cleaned_comments.append(comment.to_dict())
        
        # T·∫°o DataFrame m·ªõi
        result_df = pd.DataFrame(cleaned_comments)
        
        # Th·ªëng k√™
        valid_comments = result_df[result_df['is_valid'] == True]
        logger.info(f"Ho√†n th√†nh l√†m s·∫°ch:")
        logger.info(f"  - Comments h·ª£p l·ªá: {len(valid_comments)}/{len(result_df)}")
        logger.info(f"  - Ti·∫øng Vi·ªát: {len(result_df[result_df['language'] == 'vi'])}")
        logger.info(f"  - Ti·∫øng Anh: {len(result_df[result_df['language'] == 'en'])}")
        
        return result_df
    
    def get_cleaning_stats(self, df: pd.DataFrame) -> Dict:
        """
        L·∫•y th·ªëng k√™ v·ªÅ qu√° tr√¨nh l√†m s·∫°ch
        
        Args:
            df (pd.DataFrame): DataFrame ƒë√£ l√†m s·∫°ch
            
        Returns:
            Dict: Th·ªëng k√™
        """
        stats = {
            'total_comments': len(df),
            'valid_comments': len(df[df['is_valid'] == True]),
            'invalid_comments': len(df[df['is_valid'] == False]),
            'language_distribution': df['language'].value_counts().to_dict(),
            'avg_text_length': df['text_length'].mean(),
            'avg_word_count': df['word_count'].mean(),
            'platform_distribution': df['platform'].value_counts().to_dict() if 'platform' in df.columns else {},
            'date_range': {
                'earliest': df['published_at'].min() if 'published_at' in df.columns else None,
                'latest': df['published_at'].max() if 'published_at' in df.columns else None
            }
        }
        
        return stats


def main():
    """
    H√†m main ƒë·ªÉ test data cleaner
    """
    print("=== TEST DATA CLEANER ===\n")
    
    # T·∫°o sample data ƒë·ªÉ test
    sample_comments = [
        {
            'comment_id': '1',
            'comment_text': 'Video n√†y hay qu√°! üëçüëçüëç @username #trending https://example.com',
            'author_name': 'User1',
            'like_count': 5,
            'platform': 'YouTube'
        },
        {
            'comment_id': '2', 
            'comment_text': 'This is amazing! Love it so much ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è',
            'author_name': 'User2',
            'like_count': 10,
            'platform': 'YouTube'
        },
        {
            'comment_id': '3',
            'comment_text': 'T√¥i r·∫•t th√≠ch video n√†y, c·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª!',
            'author_name': 'User3',
            'like_count': 3,
            'platform': 'YouTube'
        },
        {
            'comment_id': '4',
            'comment_text': '',  # Comment r·ªóng
            'author_name': 'User4',
            'like_count': 0,
            'platform': 'YouTube'
        }
    ]
    
    # T·∫°o DataFrame
    df = pd.DataFrame(sample_comments)
    print("üìù Sample data:")
    print(df[['comment_text', 'author_name', 'like_count']].to_string())
    
    # Kh·ªüi t·∫°o cleaner
    cleaner = CommentDataCleaner()
    
    # L√†m s·∫°ch data
    print(f"\nüîÑ ƒêang l√†m s·∫°ch data...")
    cleaned_df = cleaner.clean_dataframe(df)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print(f"\n‚úÖ K·∫øt qu·∫£ sau khi l√†m s·∫°ch:")
    print(cleaned_df[['comment_text', 'comment_text_clean', 'language', 'is_valid', 'text_length']].to_string())
    
    # Th·ªëng k√™
    stats = cleaner.get_cleaning_stats(cleaned_df)
    print(f"\nüìä Th·ªëng k√™:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # Test c√°c h√†m ri√™ng l·∫ª
    print(f"\nüß™ Test c√°c h√†m ri√™ng l·∫ª:")
    
    test_text = "Video n√†y hay qu√°! üëçüëçüëç @username #trending https://example.com"
    print(f"Text g·ªëc: {test_text}")
    
    # Test clean_comment_text
    cleaned = cleaner.clean_comment_text(test_text)
    print(f"Sau khi l√†m s·∫°ch: {cleaned}")
    
    # Test detect_language
    language = cleaner.detect_language(cleaned)
    print(f"Ng√¥n ng·ªØ: {language}")
    
    # Test remove_stop_words
    without_stopwords = cleaner.remove_stop_words(cleaned, language)
    print(f"Kh√¥ng stop words: {without_stopwords}")


if __name__ == "__main__":
    main()
